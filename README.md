# 推荐系统

#### 📌【论文一】DIN论文需要掌握的知识

论文关键掌握：**摘要、架构、PReLU、Dice、Mini-Batch-aware-Regulazation**

- 【摘要】传统的CTR解决方式就是：使用基于深度学习的模型，将**大规模稀疏输入特征**映射为**低维向量**，然后以分组（我理解的分组就是稀疏、稠密这种）的方式**转换成定长向量**，最后**拼接**起来输入到`MLP`中，以学习特征之间的非线性关系。但是，无论候选广告是什么，用户特征都会被压缩成一个固定长度的表示向量。固定长度向量的使用将是一个瓶颈，模型难以从丰富的历史行为中有效捕捉用户的不同兴趣，于是提出`DIN`。另外，为了帮助训练近亿级别的参数量，提出`Mini-Batch-aware-Regulazation`和`data Adaptive activation function`

- 【模型架构】1️⃣`Activation Unit`: 用户历史行为`embedding1`和候选广告`embedding2`做外积，结果是一个向量；将`embedding1`，`embedding2`和输出结果三者拼接，送入全连接层，最后得到兴趣权重$w_j$（标量）。2️⃣用户行为序列中的每个行为都会产生一个权重，将产生的权重$w_j$和对应的行为$e_j$相乘，然后做`sum pooling`，这样我们就得到了基于兴趣的用户行为特征向量表示。最后我们将用户画像特征、基于兴趣的用户行为特征、候选广告、上下文特征**拼接**送入`MLP`中，`Softmax`得到结果。

  核心公式：

$$
\mathbf{v}_U(A) = f(\mathbf{v}_A, \mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_H)
= \sum_{j=1}^{H} a(\mathbf{e}_j, \mathbf{v}_A) \mathbf{e}_j
= \sum_{j=1}^{H} w_j \mathbf{e}_j
$$

- 【`PRelU`和`Dice`】1️⃣`PRelU`和`Dice`二者都有一个自适应的激活函数，也就是说都有一个**控制函数**；`PRelU`的控制函数比较简单，输入$x$大于0度时候函数值为1，$x$小于0的时候函数值为0；这也是它的缺点：分界点总是0，这是一个硬性分解【但是在实际训练中，每层输入$x$的分布是不同的，零可能不是最合适的分界点】2️⃣而`Dice`中的控制函数，是一个s型曲线，实现了软切换。工作机制：如果x比当前`mini-batch`的平均值大很多，说明该输入是高激活——p(x)趋向于1，函数趋近于f(x)=$x$；如果$x$比当前`mini-batch`的平均值小很多，说明该输入是低激活——p(x)趋向于0，函数趋近于f(x) = $α$*$x$
- 【`Mini-Batch-aware-Regulazation`】工业级广告推荐系统中，输入特征：稀疏+维度高（6亿维）；1️⃣**传统正则化(L1/L2)的挑战：** 在大规模稀疏输入的场景下，每个`mini-batch`仅激活少量参数（如几十个商品ID），大多数特征在一个`batch`中根本没出现，若强行对所有参数做L2计算，需要访问全部`Embedding`矩阵，计算开销太大。2️⃣`Mini-Batch-aware-Regulazation`：只对当前`mini-batch`中真正出现的特征对应的 `embedding`向量加正则，并做归一化处理。
- 论文地址: https://arxiv.org/pdf/1706.06978



#### 📌【论文二】SIM论文需要掌握的知识

论文关键掌握：**解决的问题、General Search Unit、Exact Search Unit**

- 【解决的问题】主要解决传统CTR模型在面对**超长用户行为序列时难以精确建模兴趣**的问题；用户历史行为序列很长就会导致：**计算量大；存储难；**1️⃣`MIMN(Memory Interaction Network)` 缺点是无法兴趣建模2️⃣`DIN`缺点是面对工业界中的用户长行为序列，**DIN 是对全量用户行为做一次 attention 计算**；计算量和存储需求是不可接受。
- 【General Search Unit】粗排；即使用简单、低成本的搜索策略，过滤掉用户无关行为。`Hard-Search`和`Soft-Search`的区别在于相关性分数的计算方式不同。1️⃣`Hard-Search`：是一种**无参数**的搜索方式；只需要判断候选商品是否和用户行为属于同一个`category`，相关行为的相关性分数就是1，不相关的相关性分数就是0；2️⃣`Soft-Search`：有参数；用户行为向量$b_i$被映射为one-hot Vector，然后embedding为低维向量$e_i$，相关性分数就是低维用户行为向量和候选商品向量做内积得到的。

![image-20250506124920977](D:\Documents\推荐系统\论文图片\Readme公式图片\image-20250506124920977.png)

- 【Exact Search Unit】精排；使用attention类的复杂模型（如DIN、DIEN）进行兴趣建模。输入是GSU的筛选结果（长度通常数百以内）
- 论文地址：https://arxiv.org/pdf/2006.05639

#### 📌【论文三】YouTuBe论文需要掌握的知识

论文关键掌握：**YouTube召回、YouTube排序、训练数据的选取和生成、Example age 是什么？**

**矩阵分解（Matrix Factorization）** 是传统协同过滤中的核心方法

为了避免视频向量过于稀疏，使用`fixed vocabulary`对视频编码嵌入为高维向量。

假设用户点了视频a，那视频a对应的label是1，其他的视频label都是0。假设总共有N个视频，那就是N个d维视频向量，用户向量是d维的，用户的d维和这N个视频向量算相似度得到N个得分，再算`softmax`得到用户对N个视频的预估分，去和label算loss，这个就是右边的训练阶段。这样就可以学出每个视频的向量，然后拿来建索引，预估的时候就是拿一个用户的向量去这个索引里找出最像的k个视频，这就是左边部分。

3.1中提到：没有使用点赞/点踩这种指标，而是将用户完整看完一个视频作为一个正样本；（因为点赞量过于稀疏，可能用户看了10个视频才点赞了一次）；②这是一个百万级别的多分类问题，作者也尝试了层次`Softmax`（Word2Vec论文中的方式，利用了霍夫曼树的思想）的方法，但是效果不好；作者采用的是负采样方式，sample出来的负样本要和正例一起`softmax`，而不是像word2vec那样做k次二分类。

3.4中提到：采用随机留出（random hold-out）会破坏用户观看视频时的时序特征；所以采用`RollBack`的预测策略。

`video vector` 是`softmax`的权重矩阵；而`user vector`是`RelU`最后一层的输出

对于`YoutubeDNN`，构造正负样本的时候要使用滑动窗口的方式去构造，其次， 是训练数据来源于用户的隐式数据， 且 **用户看完了的视频作为正样本** ， 注意这里是看完了， 有一定的时长限制， 而不是仅仅曝光点击，有可能有误点的。 而负样本，是从视频库里面随机选取，或者在曝光过的里面随机选取用户没看过的作为负样本。

**训练数据中对于每个用户选取相同的样本数， 保证用户在损失函数等权重** ， 因为这样可以减少高度活跃用户对于loss的影响。

#### 📌【论文四】POSO论文需要掌握的知识

- 论文地址：https://arxiv.org/pdf/2108.04690

#### 📌【召回评估指标】

【命中率】单个用户的前K个推荐中，只要一个命中目标商品，就算是一次命中。

【`Precision@K`】召回K个商品，用户真正喜欢的商品数量所占的比例

【召回率】用户真正喜欢的商品中，有多少被推荐在前K个中

【AP】用来评估单个用户的召回效果；和被推荐商品的排名有关

【MAP】用来评估多个用户（整个推荐系统）的召回效果；所有用户的AP相加之后求平均。它衡量推荐系统在整个测试集上的表现。

#### 📌【排序评估指标】

【DCG】也是针对**单个用户**或**单个查询**的指标。DCG综合考虑了物品能带来的收益和其所在位置的折扣效应。特别关注<span style = "color:blue">**推荐顺序**的合理性。</span>

- 物品的收益用**相关性得分**（通常是点击率、评分、点赞等）表示。**rel值越大**，表示用户对该物品的偏好越高。

$$
DCG@K = \sum_{i=1}^{K} \frac{rel_i}{\log_2(i+1)}
$$

其中：

- **K**：推荐列表的长度。  
- **rel_i**：第 \(i\) 个推荐物品的相关性得分（收益）。  
- $log_2(i+1)$：折扣因子，表示排名靠后的物品对收益的贡献降低。  

【NDCG】：对DCG归一化

【AUC】、【GAUC】：在排序、召回的过程中都会用到；AUC衡量的是模型将正样本排序在负样本之前的能力，很适合推荐系统中样本分布不平衡的这种情况。提到AUC就不得不想到ROC曲线，曲线的横坐标是假阳率，纵坐标是真阳率，也就是召回率。AUC和阈值无关，能衡量不同阈值下，模型的分类能力。但是：**AUC不能反映排序位置这个信息。**





















